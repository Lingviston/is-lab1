{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import math\n",
    "import string\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "#from nltk.stem.snowball import SnowballStemmer\n",
    "#nltk.download()\n",
    "\n",
    "from collections import Counter\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_marker = \".I\"\n",
    "header_marker = \".T\"\n",
    "authors_marker = \".A\"\n",
    "meta_marker = \".B\"\n",
    "annotation_marker = \".W\"\n",
    "\n",
    "index_type = \"index\"\n",
    "header_type = \"header\"\n",
    "authors_type = \"authors\"\n",
    "meta_type = \"meta\"\n",
    "annotation_type = \"annotation\"\n",
    "unknown_type = \"unknown\"\n",
    "\n",
    "line_types = {index_marker:\"index\", header_marker:\"header\", authors_marker:\"authors\", meta_marker:\"meta\", annotation_marker:\"annotation\"}\n",
    "\n",
    "def get_line_type(line):\n",
    "    if line.startswith(index_marker):\n",
    "        return line_types[index_marker]\n",
    "    elif line.startswith(header_marker):\n",
    "        return line_types[header_marker]\n",
    "    elif line.startswith(authors_marker):\n",
    "        return line_types[authors_marker]\n",
    "    elif line.startswith(meta_marker):\n",
    "        return line_types[meta_marker]\n",
    "    elif line.startswith(annotation_marker):\n",
    "        return line_types[annotation_marker]\n",
    "    else:\n",
    "        return unknown_type\n",
    "\n",
    "\n",
    "text_file_name = \"data/cran.all.1400\"\n",
    "documents = []\n",
    "with open(text_file_name, 'r') as file:\n",
    "    docId = 0\n",
    "    header = \"\"\n",
    "    annotation = \"\"\n",
    "    \n",
    "    is_header = False\n",
    "    is_annotation = False\n",
    "    \n",
    "    for line in file:\n",
    "        line_type = get_line_type(line)\n",
    "        if line_type == index_type:\n",
    "            is_header = False\n",
    "            is_annotation = False\n",
    "            \n",
    "            if (docId):\n",
    "                document = (docId, header, annotation)\n",
    "                documents.append(document)\n",
    "            \n",
    "            docId = line.split()[1]\n",
    "            header = \"\"\n",
    "            annotation = \"\"\n",
    "        elif line_type == header_type:\n",
    "            is_header = True\n",
    "            is_annotation = False\n",
    "        elif line_type == annotation_type:\n",
    "            is_header = False\n",
    "            is_annotation = True\n",
    "        elif line_type == authors_type or line_type == meta_type:\n",
    "            is_header = False\n",
    "            is_annotation = False\n",
    "        elif line_type == unknown_type:\n",
    "            if is_header:\n",
    "                header += line\n",
    "            elif is_annotation:\n",
    "                annotation += line\n",
    "                \n",
    "    document = (docId, header, annotation)\n",
    "    documents.append(document)\n",
    "    \n",
    "    \n",
    "queries_file_name = \"data/cran.qry\"\n",
    "queries = []\n",
    "with open(queries_file_name, 'r') as file:\n",
    "    queryId = 0\n",
    "    text = \"\"\n",
    "    \n",
    "    is_text = False\n",
    "    \n",
    "    for line in file:\n",
    "        line_type = get_line_type(line)\n",
    "        if line_type == index_type:\n",
    "            is_text = False\n",
    "            \n",
    "            if (queryId):\n",
    "                query = (queryId, text)\n",
    "                queries.append(query)\n",
    "            \n",
    "            queryId = line.split()[1]\n",
    "            text = \"\"\n",
    "        elif line_type == annotation_type:\n",
    "            is_annotation = True\n",
    "        elif line_type == unknown_type:\n",
    "            text += line\n",
    "    \n",
    "    query = (queryId, text)\n",
    "    queries.append(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize and build index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation = [c for c in string.punctuation]\n",
    "\n",
    "\n",
    "def normalize_text(text):\n",
    "    words = word_tokenize(text)\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words and word not in punctuation]\n",
    "    return words\n",
    "    \n",
    "\n",
    "def build_index(documents, index_by):\n",
    "    docs = dict()\n",
    "    index = dict()\n",
    "    \n",
    "    average_doc_lenght = 0\n",
    "    \n",
    "    for document in documents:\n",
    "        text = document[index_by]\n",
    "        normalized_text = normalize_text(text)\n",
    "        \n",
    "        doc_lenght = len(normalized_text)\n",
    "        average_doc_lenght += doc_lenght\n",
    "        \n",
    "        docId = document[0]\n",
    "        docs[docId] = doc_lenght\n",
    "            \n",
    "        words_in_doc = Counter(normalized_text)\n",
    "        for word, count in words_in_doc.items():\n",
    "            if word not in index:\n",
    "                index[word] = []\n",
    "            index[word].append((docId, count / doc_lenght))\n",
    "            \n",
    "    average_doc_lenght /= len(documents)\n",
    "    \n",
    "    return (index, docs, average_doc_lenght)\n",
    "\n",
    "\n",
    "# Index by header\n",
    "header_index, header_docs, header_avg_doc_lenght = build_index(documents, 1)\n",
    "\n",
    "# Index by annotation\n",
    "annotation_index, annotation_docs, annotation_avg_doc_lenght = build_index(documents, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search(query, index, docs_index, doc_lenght_avg):\n",
    "    search_terms = normalize_text(query)\n",
    "    \n",
    "    hits = dict()\n",
    "    for search_term in search_terms:\n",
    "        if search_term in index:\n",
    "            hits[search_term] = index[search_term]\n",
    "            \n",
    "    found_docs = dict()\n",
    "    for search_term in hits:\n",
    "        doc_tf = hits[search_term]\n",
    "        for docId, tf in doc_tf:\n",
    "            if docId not in found_docs:\n",
    "                found_docs[docId] = []\n",
    "            hit = (search_term, tf)\n",
    "            found_docs[docId].append(hit)\n",
    "    \n",
    "    N = len(docs_index)\n",
    "    k1 = 1.2\n",
    "    b = 0.75\n",
    "    result = []\n",
    "    for docId in found_docs:\n",
    "        rsv = 0\n",
    "        Ld = docs_index[docId]\n",
    "        for search_term in search_terms:\n",
    "            Nt = len(index.get(search_term, []))\n",
    "            tf = 0\n",
    "            terms_found_in_doc = found_docs[docId]\n",
    "            for search_term_in_doc, found_tf in terms_found_in_doc:\n",
    "                if (search_term_in_doc == search_term):\n",
    "                    tf = found_tf\n",
    "            if (docId == \"875\"):\n",
    "                print(search_term, Nt, tf)\n",
    "            rsv += math.log(1 + (N - Nt + 0.5) / (Nt + 0.5)) * tf * (k1 + 1) / (k1 * ((1 - b) + b * Ld / doc_lenght_avg) + tf)\n",
    "        result.append((docId, rsv))\n",
    "        \n",
    "    result.sort(key=itemgetter(1), reverse=True)\n",
    "    return result[:10]\n",
    "    \n",
    "    \n",
    "#search(\"what similarity laws must be obeyed when constructing aeroelastic models of heated high speed aircraft .\", header_index, header_docs, header_avg_doc_lenght)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity 8 0\n",
      "law 9 0\n",
      "must 0 0\n",
      "obeyed 0 0\n",
      "constructing 0 0\n",
      "aeroelastic 4 0.3333333333333333\n",
      "model 32 0.3333333333333333\n",
      "heated 7 0\n",
      "high 53 0\n",
      "speed 110 0\n",
      "aircraft 26 0\n"
     ]
    }
   ],
   "source": [
    "output_file_name = \"data/header_search_results\"\n",
    "\n",
    "with open(output_file_name, 'w') as file:\n",
    "    for idx, query in enumerate(queries):\n",
    "        search_result = search(query[1], header_index, header_docs, header_avg_doc_lenght)\n",
    "        break\n",
    "        \n",
    "        for docId, _ in search_result:\n",
    "            file.write(\"{} {}\\n\".format(idx + 1, docId))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
